{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Transformers for Next-Activity Prediction\n",
    "\n",
    "In this repository, there exist three scripts. Fill out this notebook to explain the code within them.\n",
    "\n",
    "(concept taken from https://arxiv.org/abs/2104.00721)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Architecture\n",
    "\n",
    "(explanation for the layers, type of positional encoding... suggestions of improvements maybe...)\n",
    "\n",
    "- Optional task: the inclusion of the findings in paper about positional encoding layer when dealing with time-series data: https://link.springer.com/article/10.1007/s10618-023-00948-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\M-HADI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import utils\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn import metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom layer for a single Transformer block\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        # Multi-head attention layer\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        \n",
    "        # Feed-forward neural network with regularization\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            layers.Dense(ff_dim, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
    "            layers.Dense(embed_dim, kernel_regularizer=tf.keras.regularizers.l2(1e-4))\n",
    "        ])\n",
    "        \n",
    "        # Layer normalization layers\n",
    "        self.layernorm_a = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm_b = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # Dropout layers to prevent overfitting\n",
    "        self.dropout_a = layers.Dropout(rate)\n",
    "        self.dropout_b = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        # Apply layer normalization to the inputs before attention\n",
    "        attn_input = self.layernorm_a(inputs)\n",
    "        \n",
    "        # Apply multi-head attention to the normalized inputs\n",
    "        attn_output = self.att(attn_input, attn_input)\n",
    "        \n",
    "        # Apply dropout to the attention output during training\n",
    "        attn_output = self.dropout_a(attn_output, training=training)\n",
    "        \n",
    "        # Add the original inputs to the attention output (residual connection)\n",
    "        out_a = inputs + attn_output\n",
    "        \n",
    "        # Apply layer normalization to the output of the attention layer\n",
    "        ffn_input = self.layernorm_b(out_a)\n",
    "        \n",
    "        # Apply the feed-forward network to the normalized output\n",
    "        ffn_output = self.ffn(ffn_input)\n",
    "        \n",
    "        # Apply dropout to the feed-forward output during training\n",
    "        ffn_output = self.dropout_b(ffn_output, training=training)\n",
    "        \n",
    "        # Add the residual connection and return the final output\n",
    "        return out_a + ffn_output\n",
    "\n",
    "# Define a custom layer for token and position embedding\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        \n",
    "        # Embedding layer for tokens\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        \n",
    "        # Embedding layer for positions\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Get the sequence length of the input\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        \n",
    "        # Create a tensor of positions (0, 1, 2, ..., maxlen-1)\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        \n",
    "        # Embed the positions\n",
    "        positions = self.pos_emb(positions)\n",
    "        \n",
    "        # Embed the tokens\n",
    "        x = self.token_emb(x)\n",
    "        \n",
    "        # Return the sum of token embeddings and position embeddings\n",
    "        return x + positions\n",
    "\n",
    "# Build a Transformer-based text classification model\n",
    "def get_model(max_case_length, vocab_size, output_dim, embed_dim=128, num_heads=8, ff_dim=256):\n",
    "    \"\"\"\n",
    "    Builds a Transformer-based text classification model.\n",
    "\n",
    "    Args:\n",
    "        max_case_length: Maximum length of the input sequence.\n",
    "        vocab_size: Number of words in the vocabulary.\n",
    "        output_dim: Number of output classes for classification.\n",
    "        embed_dim: Dimensionality of word embeddings (default: 128).\n",
    "        num_heads: Number of heads in the multi-head attention layer (default: 8).\n",
    "        ff_dim: Dimensionality of the feed-forward network (default: 256).\n",
    "\n",
    "    Returns:\n",
    "        A tf.keras.Model for text classification.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=(max_case_length,))\n",
    "    x = TokenAndPositionEmbedding(max_case_length, vocab_size, embed_dim)(inputs)\n",
    "    \n",
    "    # Add multiple transformer blocks\n",
    "    for _ in range(2):  # Increased number of transformer blocks\n",
    "        x = TransformerBlock(embed_dim, num_heads, ff_dim)(x, training=True)\n",
    "        \n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(0.3)(x)  # Slightly increased dropout rate\n",
    "    x = layers.Dense(256, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)  # Increased dense layer size\n",
    "    x = layers.Dropout(0.3)(x)  # Slightly increased dropout rate\n",
    "    outputs = layers.Dense(output_dim, activation=\"linear\")(x)\n",
    "    \n",
    "    transformer = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"improved_transformer\")\n",
    "    return transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TransformerBlock Class**\n",
    "\n",
    "- Initialization (__init__ method):\n",
    "\n",
    "    - Multi-Head Attention Layer (self.att): This layer performs attention over multiple heads to capture different aspects of the input sequence. It uses num_heads and embed_dim to configure the attention mechanism.\n",
    "    - Feed-Forward Network (self.ffn): This is a small neural network consisting of two dense layers with a ReLU activation in between. It also includes L2 regularization to prevent overfitting.\n",
    "    - Layer Normalization (self.layernorm_a, self.layernorm_b): These layers normalize the input to have zero mean and unit variance, helping in stabilizing and speeding up the training process.\n",
    "    - Dropout Layers (self.dropout_a, self.dropout_b): These layers randomly drop a fraction of the input units during training to prevent overfitting.\n",
    "\n",
    "- Forward Pass (call method):\n",
    "\n",
    "    - Layer Normalization (attn_input): Normalizes the inputs before applying attention.\n",
    "    - Multi-Head Attention (attn_output): Computes the attention scores and outputs the attended values.\n",
    "- Dropout (self.dropout_a): Applies dropout to the attention output during training.\n",
    "- Residual Connection (out_a): Adds the original input to the attention output to form a residual connection.\n",
    "- Layer Normalization (ffn_input): Normalizes the output of the residual connection.\n",
    "- Feed-Forward Network (ffn_output): Applies the feed-forward neural network to the normalized output.\n",
    "- Dropout (self.dropout_b): Applies dropout to the feed-forward network output during training.\n",
    "- Residual Connection: Adds the normalized output of the residual connection to the feed-forward network output and returns it.\n",
    "\n",
    "**TokenAndPositionEmbedding Class**\n",
    "\n",
    "- Initialization (__init__ method):\n",
    "\n",
    "    - Token Embedding (self.token_emb): Embeds each token (word or character) in the input sequence into a dense vector of fixed size (embed_dim).\n",
    "    - Position Embedding (self.pos_emb): Embeds each position in the input sequence into a dense vector of the same size (embed_dim).\n",
    "\n",
    "- Forward Pass (call method):\n",
    "\n",
    "    - Sequence Length (maxlen): Determines the length of the input sequence dynamically.\n",
    "    - Position Tensor (positions): Creates a tensor representing the positions in the input sequence.\n",
    "    - Position Embedding (positions): Converts the position tensor into position embeddings.\n",
    "    - Token Embedding (x): Converts the input tokens into token embeddings.\n",
    "    - Sum of Embeddings: Adds the position embeddings to the token embeddings to form the final input representation.\n",
    "\n",
    "\n",
    "- **TokenAndPositionEmbedding:**\n",
    "    - Word Embeddings: Capture the meaning of a word (learned from text data). Similar words have similar embeddings.\n",
    "    - Positional Embeddings: Encode word order (vectors change based on position in the sequence).\n",
    " \n",
    "#### Function:\n",
    "- **get_model** :\n",
    "- 1. Input Layer (layers.Input(shape=(max_case_length,))):\n",
    "    - Defines the starting point for the model, accepting sequences of integers representing word indices in the text data.\n",
    "    - max_case_length controls the maximum number of words allowed in a sentence.\n",
    "- 2. Token and Position Embedding (x = TokenAndPositionEmbedding(...)):\n",
    "    - Transforms integer sequences into a richer format by combining:\n",
    "    - Word Embeddings: Capture the meaning of each word (learned from text data).\n",
    "    - Positional Embeddings: Encode the order of words in the sentence.\n",
    "- 3. Transformer Blocks (for _ in range(4): ...):\n",
    "    - Stacks four TransformerBlock instances, the core building blocks:\n",
    "    - Multi-head Attention: Analyzes relationships between words in the sentence.\n",
    "    - Feed-forward Network (FFN): Captures complex patterns beyond word relationships.\n",
    "    - Normalization and Dropout: Stabilizes training and prevents overfitting.\n",
    "- 4. Sequence Pooling (x = layers.GlobalAveragePooling1D()(x)):\n",
    "  - Summarizes the entire sentence by averaging the outputs from all elements in the sequence.\n",
    "- 5. Classification Head (layers.Dense(...)...):\n",
    "\n",
    "    - Predicts the class label for the text:\n",
    "    - Dense layer (256 units): Projects features to a higher dimension.\n",
    "    - Dropout (0.3): Prevents overfitting by randomly dropping information during training.\n",
    "    - Dense layer (output_dim units): Generates final output scores for classification.\n",
    "- 6. Model Creation (transformer = tf.keras.Model(...)):\n",
    "\n",
    "Combines all the steps into a single tf.keras.Model instance named \"improved_transformer\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "(What is the dataset, what is the vocabulary size, why is it processed the way it is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogsDataLoader:\n",
    "    def __init__(self, name, dir_path = \"./datasets\"):\n",
    "        \"\"\"Provides support for reading and \n",
    "            pre-processing examples from processed logs.\n",
    "        Args:\n",
    "            name: str: name of the dataset as used during processing raw logs\n",
    "            dir_path: str: Path to dataset directory\n",
    "        \"\"\"\n",
    "        self._dir_path = f\"{dir_path}/{name}/processed\"\n",
    "\n",
    "    def prepare_data_next_activity(self, df, \n",
    "        x_word_dict, y_word_dict, \n",
    "        max_case_length, shuffle=True):\n",
    "        \n",
    "        x = df[\"prefix\"].values\n",
    "        y = df[\"next_act\"].values\n",
    "        if shuffle:\n",
    "            x, y = utils.shuffle(x, y)\n",
    "\n",
    "        token_x = list()\n",
    "        for _x in x:\n",
    "            token_x.append([x_word_dict[s] for s in _x.split()])\n",
    "        # token_x = np.array(token_x, dtype = np.float32)\n",
    "\n",
    "        token_y = list()\n",
    "        for _y in y:\n",
    "            token_y.append(y_word_dict[_y])\n",
    "        # token_y = np.array(token_y, dtype = np.float32)\n",
    "\n",
    "        token_x = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            token_x, maxlen=max_case_length)\n",
    "\n",
    "        token_x = np.array(token_x, dtype=np.float32)\n",
    "        token_y = np.array(token_y, dtype=np.float32)\n",
    "\n",
    "        return token_x, token_y\n",
    "\n",
    "    def get_max_case_length(self, train_x):\n",
    "        train_token_x = list()\n",
    "        for _x in train_x:\n",
    "            train_token_x.append(len(_x.split()))\n",
    "        return max(train_token_x)\n",
    "\n",
    "    def load_data(self, task):\n",
    "        train_df = pd.read_csv(f\"{self._dir_path}/{task}_train.csv\")\n",
    "        test_df = pd.read_csv(f\"{self._dir_path}/{task}_test.csv\")\n",
    "\n",
    "        with open(f\"{self._dir_path}/metadata.json\", \"r\") as json_file:\n",
    "            metadata = json.load(json_file)\n",
    "\n",
    "        x_word_dict = metadata[\"x_word_dict\"]\n",
    "        y_word_dict = metadata[\"y_word_dict\"]\n",
    "        max_case_length = self.get_max_case_length(train_df[\"prefix\"].values)\n",
    "        vocab_size = len(x_word_dict) \n",
    "        total_classes = len(y_word_dict)\n",
    "\n",
    "        return (train_df, test_df, \n",
    "            x_word_dict, y_word_dict, \n",
    "            max_case_length, vocab_size, \n",
    "            total_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LogsDataLoader for loading and pre-processing sequential data from processed log files.\n",
    "\n",
    "    - It takes a dataset name and directory path during initialization.\n",
    "    - The prepare_data_next_activity function prepares training data by converting text sequences to numerical arrays and padding them to a fixed length.\n",
    "    - It can also find the maximum sequence length in the data using get_max_case_length.\n",
    "    - The load_data function loads training and testing dataframes, reads metadata (word dictionaries), and calculates additional parameters liken vocabulary size and number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Using The Model\n",
    "\n",
    "(model compilation, loss function, accuracy metrics, data loading...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expermenting with 4 transformers bloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\M-HADI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\M-HADI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "1107/1107 - 55s - loss: 0.9429 - sparse_categorical_accuracy: 0.7749 - 55s/epoch - 50ms/step\n",
      "Epoch 2/10\n",
      "1107/1107 - 51s - loss: 0.7630 - sparse_categorical_accuracy: 0.8131 - 51s/epoch - 46ms/step\n",
      "Epoch 3/10\n",
      "1107/1107 - 56s - loss: 0.7141 - sparse_categorical_accuracy: 0.8175 - 56s/epoch - 51ms/step\n",
      "Epoch 4/10\n",
      "1107/1107 - 53s - loss: 0.6962 - sparse_categorical_accuracy: 0.8140 - 53s/epoch - 48ms/step\n",
      "Epoch 5/10\n",
      "1107/1107 - 56s - loss: 0.6621 - sparse_categorical_accuracy: 0.8159 - 56s/epoch - 51ms/step\n",
      "Epoch 6/10\n",
      "1107/1107 - 55s - loss: 0.6460 - sparse_categorical_accuracy: 0.8162 - 55s/epoch - 50ms/step\n",
      "Epoch 7/10\n",
      "1107/1107 - 52s - loss: 0.6294 - sparse_categorical_accuracy: 0.8166 - 52s/epoch - 47ms/step\n",
      "Epoch 8/10\n",
      "1107/1107 - 53s - loss: 0.6357 - sparse_categorical_accuracy: 0.8152 - 53s/epoch - 48ms/step\n",
      "Epoch 9/10\n",
      "1107/1107 - 52s - loss: 0.6171 - sparse_categorical_accuracy: 0.8191 - 52s/epoch - 47ms/step\n",
      "Epoch 10/10\n",
      "1107/1107 - 53s - loss: 0.6243 - sparse_categorical_accuracy: 0.8165 - 53s/epoch - 48ms/step\n",
      "29/29 [==============================] - 2s 35ms/step\n",
      "29/29 [==============================] - 1s 34ms/step\n",
      "28/28 [==============================] - 1s 35ms/step\n",
      "17/17 [==============================] - 1s 42ms/step\n",
      "5/5 [==============================] - 0s 34ms/step\n",
      "2/2 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Average accuracy across all prefixes: 0.6895899164731054\n",
      "Average f-score across all prefixes: 0.639308405932111\n",
      "Average precision across all prefixes: 0.6360754003058541\n",
      "Average recall across all prefixes: 0.6657122177402675\n"
     ]
    }
   ],
   "source": [
    "dataset=\"helpdesk\"\n",
    "model_dir=\"./models\"\n",
    "result_dir=\"./results\"\n",
    "task = \"next_activity\"\n",
    "\n",
    "epochs=10\n",
    "batch_size=12\n",
    "learning_rate=0.001\n",
    "gpu=0\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create directories to save the results and models\n",
    "    model_path = f\"{model_dir}/{dataset}\"\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "    model_path = f\"{model_path}/next_activity_ckpt.weights.h5\"\n",
    "\n",
    "    result_path = f\"{result_dir}/{dataset}\"\n",
    "    if not os.path.exists(result_path):\n",
    "        os.makedirs(result_path)\n",
    "    result_path = f\"{result_path}/results\"\n",
    "\n",
    "    # Load data\n",
    "    data_loader = LogsDataLoader(name=dataset)\n",
    "\n",
    "    (train_df, test_df, x_word_dict, y_word_dict, max_case_length, \n",
    "        vocab_size, num_output) = data_loader.load_data(task)\n",
    "    \n",
    "    # Prepare training examples for next activity prediction task\n",
    "    train_token_x, train_token_y = data_loader.prepare_data_next_activity(train_df, \n",
    "        x_word_dict, y_word_dict, max_case_length)\n",
    "    \n",
    "    # Create and train a transformer model\n",
    "    transformer_model = get_model(\n",
    "        max_case_length=max_case_length, \n",
    "        vocab_size=vocab_size,\n",
    "        output_dim=num_output)\n",
    "\n",
    "    transformer_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=model_path,\n",
    "        save_weights_only=True,\n",
    "        monitor=\"sparse_categorical_accuracy\",\n",
    "        mode=\"max\", save_best_only=True)\n",
    "\n",
    "\n",
    "    transformer_model.fit(train_token_x, train_token_y, \n",
    "        epochs=epochs, batch_size=batch_size, \n",
    "        shuffle=True, verbose=2, callbacks=[model_checkpoint_callback])\n",
    "\n",
    "    # Evaluate over all the prefixes (k) and save the results\n",
    "    k, accuracies,fscores, precisions, recalls = [],[],[],[],[]\n",
    "    for i in range(max_case_length):\n",
    "        test_data_subset = test_df[test_df[\"k\"]==i]\n",
    "        if len(test_data_subset) > 0:\n",
    "            test_token_x, test_token_y = data_loader.prepare_data_next_activity(test_data_subset, \n",
    "                x_word_dict, y_word_dict, max_case_length)   \n",
    "            y_pred = np.argmax(transformer_model.predict(test_token_x), axis=1)\n",
    "            accuracy = metrics.accuracy_score(test_token_y, y_pred)\n",
    "            precision, recall, fscore, _ = metrics.precision_recall_fscore_support(\n",
    "                test_token_y, y_pred, average=\"weighted\")\n",
    "            k.append(i)\n",
    "            accuracies.append(accuracy)\n",
    "            fscores.append(fscore)\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "\n",
    "    k.append(i + 1)\n",
    "    accuracies.append(np.mean(accuracy))\n",
    "    fscores.append(np.mean(fscores))\n",
    "    precisions.append(np.mean(precisions))\n",
    "    recalls.append(np.mean(recalls))\n",
    "    print('Average accuracy across all prefixes:', np.mean(accuracies))\n",
    "    print('Average f-score across all prefixes:', np.mean(fscores))\n",
    "    print('Average precision across all prefixes:', np.mean(precisions))\n",
    "    print('Average recall across all prefixes:', np.mean(recalls))    \n",
    "    results_df = pd.DataFrame({\"k\":k, \"accuracy\":accuracies, \"fscore\": fscores, \n",
    "        \"precision\":precisions, \"recall\":recalls})\n",
    "    results_df.to_csv(result_path+\"_next_activity.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expermenting with 10 epochs and 2 transformers block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1107/1107 - 35s - loss: 0.8040 - sparse_categorical_accuracy: 0.7896 - 35s/epoch - 32ms/step\n",
      "Epoch 2/10\n",
      "1107/1107 - 31s - loss: 0.6947 - sparse_categorical_accuracy: 0.8147 - 31s/epoch - 28ms/step\n",
      "Epoch 3/10\n",
      "1107/1107 - 30s - loss: 0.6775 - sparse_categorical_accuracy: 0.8157 - 30s/epoch - 27ms/step\n",
      "Epoch 4/10\n",
      "1107/1107 - 32s - loss: 0.6525 - sparse_categorical_accuracy: 0.8141 - 32s/epoch - 29ms/step\n",
      "Epoch 5/10\n",
      "1107/1107 - 32s - loss: 0.6369 - sparse_categorical_accuracy: 0.8167 - 32s/epoch - 29ms/step\n",
      "Epoch 6/10\n",
      "1107/1107 - 29s - loss: 0.6219 - sparse_categorical_accuracy: 0.8172 - 29s/epoch - 26ms/step\n",
      "Epoch 7/10\n",
      "1107/1107 - 29s - loss: 0.6146 - sparse_categorical_accuracy: 0.8166 - 29s/epoch - 26ms/step\n",
      "Epoch 8/10\n",
      "1107/1107 - 29s - loss: 0.6122 - sparse_categorical_accuracy: 0.8145 - 29s/epoch - 26ms/step\n",
      "Epoch 9/10\n",
      "1107/1107 - 29s - loss: 0.6087 - sparse_categorical_accuracy: 0.8175 - 29s/epoch - 26ms/step\n",
      "Epoch 10/10\n",
      "1107/1107 - 30s - loss: 0.5995 - sparse_categorical_accuracy: 0.8165 - 30s/epoch - 27ms/step\n",
      "29/29 [==============================] - 1s 18ms/step\n",
      "29/29 [==============================] - 1s 19ms/step\n",
      "28/28 [==============================] - 1s 23ms/step\n",
      "17/17 [==============================] - 0s 21ms/step\n",
      "5/5 [==============================] - 0s 21ms/step\n",
      "2/2 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Average accuracy across all prefixes: 0.5713509359002252\n",
      "Average f-score across all prefixes: 0.5093383203932483\n",
      "Average precision across all prefixes: 0.4994833942917989\n",
      "Average recall across all prefixes: 0.5383779309694733\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "# Create and train a transformer model\n",
    "transformer_model = get_model(\n",
    "    max_case_length=max_case_length, \n",
    "    vocab_size=vocab_size,\n",
    "    output_dim=num_output)\n",
    "\n",
    "transformer_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=model_path,\n",
    "    save_weights_only=True,\n",
    "    monitor=\"sparse_categorical_accuracy\",\n",
    "    mode=\"max\", save_best_only=True)\n",
    "\n",
    "\n",
    "transformer_model.fit(train_token_x, train_token_y, \n",
    "    epochs=epochs, batch_size=batch_size, \n",
    "    shuffle=True, verbose=2, callbacks=[model_checkpoint_callback])\n",
    "\n",
    "# Evaluate over all the prefixes (k) and save the results\n",
    "k, accuracies,fscores, precisions, recalls = [],[],[],[],[]\n",
    "for i in range(max_case_length):\n",
    "    test_data_subset = test_df[test_df[\"k\"]==i]\n",
    "    if len(test_data_subset) > 0:\n",
    "        test_token_x, test_token_y = data_loader.prepare_data_next_activity(test_data_subset, \n",
    "            x_word_dict, y_word_dict, max_case_length)   \n",
    "        y_pred = np.argmax(transformer_model.predict(test_token_x), axis=1)\n",
    "        accuracy = metrics.accuracy_score(test_token_y, y_pred)\n",
    "        precision, recall, fscore, _ = metrics.precision_recall_fscore_support(\n",
    "            test_token_y, y_pred, average=\"weighted\")\n",
    "        k.append(i)\n",
    "        accuracies.append(accuracy)\n",
    "        fscores.append(fscore)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "k.append(i + 1)\n",
    "accuracies.append(np.mean(accuracy))\n",
    "fscores.append(np.mean(fscores))\n",
    "precisions.append(np.mean(precisions))\n",
    "recalls.append(np.mean(recalls))\n",
    "print('Average accuracy across all prefixes:', np.mean(accuracies))\n",
    "print('Average f-score across all prefixes:', np.mean(fscores))\n",
    "print('Average precision across all prefixes:', np.mean(precisions))\n",
    "print('Average recall across all prefixes:', np.mean(recalls))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expermenting with 20 epochs and 2 transformers block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1107/1107 - 30s - loss: 0.8050 - sparse_categorical_accuracy: 0.7916 - 30s/epoch - 27ms/step\n",
      "Epoch 2/20\n",
      "1107/1107 - 28s - loss: 0.6961 - sparse_categorical_accuracy: 0.8169 - 28s/epoch - 26ms/step\n",
      "Epoch 3/20\n",
      "1107/1107 - 26s - loss: 0.6686 - sparse_categorical_accuracy: 0.8164 - 26s/epoch - 23ms/step\n",
      "Epoch 4/20\n",
      "1107/1107 - 26s - loss: 0.6549 - sparse_categorical_accuracy: 0.8141 - 26s/epoch - 23ms/step\n",
      "Epoch 5/20\n",
      "1107/1107 - 29s - loss: 0.6325 - sparse_categorical_accuracy: 0.8170 - 29s/epoch - 27ms/step\n",
      "Epoch 6/20\n",
      "1107/1107 - 29s - loss: 0.6237 - sparse_categorical_accuracy: 0.8168 - 29s/epoch - 26ms/step\n",
      "Epoch 7/20\n",
      "1107/1107 - 27s - loss: 0.6101 - sparse_categorical_accuracy: 0.8177 - 27s/epoch - 24ms/step\n",
      "Epoch 8/20\n",
      "1107/1107 - 29s - loss: 0.6082 - sparse_categorical_accuracy: 0.8171 - 29s/epoch - 26ms/step\n",
      "Epoch 9/20\n",
      "1107/1107 - 29s - loss: 0.6011 - sparse_categorical_accuracy: 0.8180 - 29s/epoch - 26ms/step\n",
      "Epoch 10/20\n",
      "1107/1107 - 29s - loss: 0.6020 - sparse_categorical_accuracy: 0.8159 - 29s/epoch - 26ms/step\n",
      "Epoch 11/20\n",
      "1107/1107 - 30s - loss: 0.5965 - sparse_categorical_accuracy: 0.8167 - 30s/epoch - 27ms/step\n",
      "Epoch 12/20\n",
      "1107/1107 - 32s - loss: 0.5935 - sparse_categorical_accuracy: 0.8165 - 32s/epoch - 29ms/step\n",
      "Epoch 13/20\n",
      "1107/1107 - 32s - loss: 0.5906 - sparse_categorical_accuracy: 0.8190 - 32s/epoch - 29ms/step\n",
      "Epoch 14/20\n",
      "1107/1107 - 31s - loss: 0.5861 - sparse_categorical_accuracy: 0.8174 - 31s/epoch - 28ms/step\n",
      "Epoch 15/20\n",
      "1107/1107 - 32s - loss: 0.5874 - sparse_categorical_accuracy: 0.8197 - 32s/epoch - 29ms/step\n",
      "Epoch 16/20\n",
      "1107/1107 - 30s - loss: 0.5867 - sparse_categorical_accuracy: 0.8177 - 30s/epoch - 27ms/step\n",
      "Epoch 17/20\n",
      "1107/1107 - 29s - loss: 0.5824 - sparse_categorical_accuracy: 0.8172 - 29s/epoch - 27ms/step\n",
      "Epoch 18/20\n",
      "1107/1107 - 31s - loss: 0.5843 - sparse_categorical_accuracy: 0.8191 - 31s/epoch - 28ms/step\n",
      "Epoch 19/20\n",
      "1107/1107 - 31s - loss: 0.5792 - sparse_categorical_accuracy: 0.8180 - 31s/epoch - 28ms/step\n",
      "Epoch 20/20\n",
      "1107/1107 - 31s - loss: 0.5798 - sparse_categorical_accuracy: 0.8195 - 31s/epoch - 28ms/step\n",
      "29/29 [==============================] - 1s 23ms/step\n",
      "29/29 [==============================] - 1s 22ms/step\n",
      "28/28 [==============================] - 1s 21ms/step\n",
      "17/17 [==============================] - 0s 23ms/step\n",
      "5/5 [==============================] - 0s 25ms/step\n",
      "2/2 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Average accuracy across all prefixes: 0.6121053036607403\n",
      "Average f-score across all prefixes: 0.5565533079938956\n",
      "Average precision across all prefixes: 0.563432334876684\n",
      "Average recall across all prefixes: 0.5822672500961819\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "# Create and train a transformer model\n",
    "transformer_model = get_model(\n",
    "    max_case_length=max_case_length, \n",
    "    vocab_size=vocab_size,\n",
    "    output_dim=num_output)\n",
    "\n",
    "transformer_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=model_path,\n",
    "    save_weights_only=True,\n",
    "    monitor=\"sparse_categorical_accuracy\",\n",
    "    mode=\"max\", save_best_only=True)\n",
    "\n",
    "\n",
    "transformer_model.fit(train_token_x, train_token_y, \n",
    "    epochs=epochs, batch_size=batch_size, \n",
    "    shuffle=True, verbose=2, callbacks=[model_checkpoint_callback])\n",
    "\n",
    "# Evaluate over all the prefixes (k) and save the results\n",
    "k, accuracies,fscores, precisions, recalls = [],[],[],[],[]\n",
    "for i in range(max_case_length):\n",
    "    test_data_subset = test_df[test_df[\"k\"]==i]\n",
    "    if len(test_data_subset) > 0:\n",
    "        test_token_x, test_token_y = data_loader.prepare_data_next_activity(test_data_subset, \n",
    "            x_word_dict, y_word_dict, max_case_length)   \n",
    "        y_pred = np.argmax(transformer_model.predict(test_token_x), axis=1)\n",
    "        accuracy = metrics.accuracy_score(test_token_y, y_pred)\n",
    "        precision, recall, fscore, _ = metrics.precision_recall_fscore_support(\n",
    "            test_token_y, y_pred, average=\"weighted\")\n",
    "        k.append(i)\n",
    "        accuracies.append(accuracy)\n",
    "        fscores.append(fscore)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "k.append(i + 1)\n",
    "accuracies.append(np.mean(accuracy))\n",
    "fscores.append(np.mean(fscores))\n",
    "precisions.append(np.mean(precisions))\n",
    "recalls.append(np.mean(recalls))\n",
    "print('Average accuracy across all prefixes:', np.mean(accuracies))\n",
    "print('Average f-score across all prefixes:', np.mean(fscores))\n",
    "print('Average precision across all prefixes:', np.mean(precisions))\n",
    "print('Average recall across all prefixes:', np.mean(recalls))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suggestions for Improvements**\n",
    "\n",
    "**Positional Encoding:**\n",
    "\n",
    "- Currently, the model uses learnable position embeddings. An alternative is to use sinusoidal position embeddings, as proposed in the original Transformer paper. Sinusoidal embeddings have the advantage of being fixed and potentially providing better inductive biases for the model.\n",
    "\n",
    "**Pre-trained Embeddings:**\n",
    "- Use pre-trained embeddings (e.g., GloVe, Word2Vec, or BERT embeddings) for the token embeddings. This can help the model leverage external knowledge and improve performance, especially when training data is limited.\n",
    "\n",
    "**Transformer Block Hyperparameters:**\n",
    "- Experiment with different numbers of heads (num_heads) and different feed-forward network dimensions (ff_dim) to find the best configuration for your specific dataset.\n",
    "\n",
    "**Layer Scaling:**\n",
    "- Apply layer scaling (e.g., LayerScale) to improve training stability and performance, especially for deeper models.\n",
    "\n",
    "**Learning Rate Schedules:**\n",
    "- Use advanced learning rate schedules such as the learning rate warm-up followed by a cosine decay to improve training efficiency and convergence.\n",
    "\n",
    "**Regularization:**\n",
    "- Incorporate other regularization techniques such as weight decay and layer-wise adaptive rate scaling (LARS) to further prevent overfitting and improve generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
